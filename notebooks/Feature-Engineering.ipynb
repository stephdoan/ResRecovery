{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from utils import *\n",
    "from features import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbit_rate = 1/125000\n",
    "\n",
    "low_fp = '../data/240p/' \n",
    "med_fp = '../data/480p/'\n",
    "high_fp = '../data/1080p/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dfs = []\n",
    "for file in os.listdir(low_fp):\n",
    "    if file != '.ipynb_checkpoints':\n",
    "        low_dfs.append(pd.read_csv(low_fp+file))\n",
    "    \n",
    "med_dfs = []\n",
    "for file in os.listdir(med_fp):\n",
    "    if file != '.ipynb_checkpoints':\n",
    "        med_dfs.append(pd.read_csv(med_fp+file))\n",
    "    \n",
    "high_dfs = []\n",
    "for file in os.listdir(high_fp):\n",
    "    if file != '.ipynb_checkpoints':\n",
    "        high_dfs.append(pd.read_csv(high_fp+file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the aggregate features of the whole chunk; download and upload\n",
    "def agg_feat(df, col):\n",
    "  return [np.mean(df[col]), np.std(df[col])]\n",
    "\n",
    "## take the ratio of upload:download packets\n",
    "def pkt_ratio(df):\n",
    "  ms_df = convert_ms_df(df, True)\n",
    "  local = np.sum(ms_df['pkt_src'] == '1') \n",
    "  server = np.sum(ms_df['pkt_src'] == '2') \n",
    "  return local / server\n",
    "\n",
    "## take the ratio of upload:download bytes\n",
    "def bytes_ratio(df):\n",
    "  local = df['1->2Bytes'].sum()\n",
    "  server = df['2->1Bytes'].sum()\n",
    "  return local / server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Related Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finds the peaks with mean + 2(1) std\n",
    "## run the above aggregate functions on the peaks only??\n",
    "\n",
    "def get_peak_loc(df, col, invert=False):\n",
    "  'invert arg allows you to get values not considered peaks'\n",
    "  df_avg = df[col].mean()\n",
    "  df_std = df[col].std()\n",
    "  \n",
    "  threshold = df_avg + (1 * df_std)\n",
    "  \n",
    "  if invert:\n",
    "    return np.array(df[col] < threshold)\n",
    "  \n",
    "  else:\n",
    "    return np.array(df[col] > threshold)\n",
    "\n",
    "## np.mean, np.var, np.std - think of more?  \n",
    "def peak_time_diff(df, col):\n",
    "  '''\n",
    "  mess around with the different inputs for function. \n",
    "  variance seems to inflate the difference betweent the two the most with litte\n",
    "  to no data manipulation. however, currently trying things like\n",
    "  squaring the data before taking the aggregate function to exaggerate\n",
    "  differences (moderate success??)\n",
    "  '''\n",
    "  peaks = df[get_peak_loc(df, col)]\n",
    "  peaks['Time'] = peaks['Time'] - peaks['Time'].min()\n",
    "  time_diff = np.diff(peaks['Time'] ** 2)\n",
    "  return [np.mean(time_diff), np.var(time_diff), np.std(time_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def peak_times(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    if list(peaks) == []:\n",
    "        return [-1]\n",
    "    times = df.iloc[peaks]['Time'].values\n",
    "    time_between_peaks = [times[i]-times[i-1]for i in range(1,len(times))]\n",
    "    #print(time_between_peaks)\n",
    "    #time_between_peaks[0]=0\n",
    "    if time_between_peaks == []:\n",
    "        return -1\n",
    "    return time_between_peaks\n",
    "\n",
    "def num_peaks(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    return len(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_features(df, col):\n",
    "\n",
    "    \"\"\"\n",
    "    welch implemention of spectral features\n",
    "    resample the data before inputting (might change prereq depending on\n",
    "    resource allocation)\n",
    "    \"\"\"\n",
    "\n",
    "    f, Pxx_den = sp.signal.welch(df[col], fs=2)\n",
    "    Pxx_den = np.sqrt(Pxx_den)\n",
    "\n",
    "    peaks = sp.signal.find_peaks(Pxx_den)[0]\n",
    "    prominences = sp.signal.peak_prominences(Pxx_den, peaks)[0]\n",
    "\n",
    "    idx_max = prominences.argmax()\n",
    "    loc_max = peaks[idx_max]\n",
    "\n",
    "    return [f[loc_max], Pxx_den[loc_max], prominences[idx_max]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wip; need to decide chunk size eventually\n",
    "## should we also make this chunking feature be our feature creation?\n",
    "\n",
    "def chunk_data(df, interval=60):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in a filepath to the data you want to chunk and feature engineer\n",
    "    chunks our data into a specified time interval\n",
    "    each chunk is then turned into an observation to be fed into our classifier\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    df['Time'] = df['Time'] - df['Time'].min()\n",
    "    \n",
    "    total_chunks = np.floor(df['Time'].max() / interval).astype(int)\n",
    "\n",
    "    for chunk in np.arange(total_chunks):\n",
    "      \n",
    "        start = chunk * interval\n",
    "        end = (chunk+1) * interval\n",
    "\n",
    "        temp_df = (df[(df['Time'] >= start) & (df['Time'] < end)])\n",
    "        \n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'dwl_peak_freq',\n",
    "    'dwl_peak_prom',\n",
    "    'dwl_max_psd',\n",
    "    'dwl_bytes_avg',\n",
    "    'dwl_bytes_std',\n",
    "    'dwl_peak_avg',\n",
    "    'dwl_peak_var',\n",
    "    'dwl_peak_std',\n",
    "    'upl_peak_freq',\n",
    "    'upl_peak_prom',\n",
    "    'upl_max_psd',\n",
    "    'upl_bytes_avg',\n",
    "    'upl_bytes_std',\n",
    "    'upl_peak_avg',\n",
    "    'upl_peak_var',\n",
    "    'upl_peak_std',\n",
    "    'IMAN_dwn_time_peak',#'IMAN_up_time_peak',\n",
    "    'IMAN_dwn_num_peak'#,'IMAN_up_num_peak'\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "    upl_bytes = preproc[preproc['pkt_src'] == '1'].resample('500ms', on='Time').sum()\n",
    "    dwl_bytes = preproc[preproc['pkt_src'] == '2'].resample('500ms', on='Time').sum()\n",
    "\n",
    "    ## spectral features\n",
    "    dwl_spectral = spectral_features(dwl_bytes, 'pkt_size')\n",
    "    upl_spectral = spectral_features(upl_bytes, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    dwl_agg = agg_feat(chunk, '2->1Bytes')\n",
    "    upl_agg = agg_feat(chunk, '1->2Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    dwl_peak = peak_time_diff(chunk, '2->1Bytes')\n",
    "    upl_peak = peak_time_diff(chunk, '1->2Bytes')\n",
    "    \n",
    "    ## iman's time between peak \n",
    "    iman_dwn_time_peak = np.mean(peak_times(chunk,'2->1Bytes',1000000))\n",
    "    #iman_up_time_peak = np.mean(peak_times(chunk,'1->2Bytes',50000))\n",
    "\n",
    "    ## iman's num peak\n",
    "    iman_dwn_num_peak = num_peaks(chunk,'2->1Bytes',1000000)\n",
    "    #iman_up_num_peak = num_peaks(chunk,'1->2Bytes',50000)\n",
    "\n",
    "    feat_val = np.hstack((\n",
    "      dwl_spectral,\n",
    "      dwl_agg,\n",
    "      dwl_peak,\n",
    "      upl_spectral,\n",
    "      upl_agg,\n",
    "      upl_peak,\n",
    "      iman_dwn_time_peak,\n",
    "      #iman_up_time_peak,\n",
    "      iman_dwn_num_peak,\n",
    "      #iman_up_num_peak\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_no_split(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'peak_freq',\n",
    "    'peak_prom',\n",
    "    'max_psd',\n",
    "    'bytes_avg',\n",
    "    'bytes_std',\n",
    "    'peak_avg',\n",
    "    'peak_var',\n",
    "    'peak_std',\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "\n",
    "    ## spectral features\n",
    "    spectral_feat = spectral_features(preproc, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    aggr_feat = agg_feat(chunk, '2->1Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    peak_feat = peak_time_diff(chunk, '2->1Bytes')\n",
    "    \n",
    "    feat_val = np.hstack((\n",
    "      spectral_feat,\n",
    "      aggr_feat,\n",
    "      peak_feat\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 45.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "low_feat = pd.concat([create_features(df, 100) for df in low_dfs])\n",
    "med_feat = pd.concat([create_features(df, 100) for df in med_dfs])\n",
    "high_feat = pd.concat([create_features(df, 100) for df in high_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = pd.concat([low_feat, med_feat, high_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_split.drop(columns=['resolution']), training_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_split = RandomForestClassifier(n_estimators = 5, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resolution            1.000000\n",
       "dwl_max_psd           0.642118\n",
       "dwl_peak_prom         0.640417\n",
       "IMAN_dwn_num_peak     0.605758\n",
       "dwl_bytes_avg         0.598011\n",
       "dwl_bytes_std         0.582058\n",
       "upl_peak_std          0.434709\n",
       "upl_peak_var          0.395590\n",
       "dwl_peak_std          0.391721\n",
       "upl_bytes_avg         0.386697\n",
       "dwl_peak_var          0.383599\n",
       "dwl_peak_avg          0.371300\n",
       "upl_peak_avg          0.356662\n",
       "upl_peak_prom         0.315430\n",
       "upl_max_psd           0.314997\n",
       "upl_bytes_std         0.314364\n",
       "IMAN_dwn_time_peak    0.235416\n",
       "dwl_peak_freq         0.161576\n",
       "upl_peak_freq         0.026870\n",
       "Name: resolution, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(training_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Resolution</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Resolution</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Resolution  0.0  1.0  2.0\n",
       "Actual Resolution                  \n",
       "0.0                    14    1    0\n",
       "1.0                     3    8    0\n",
       "2.0                     2    0   12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82352941, 0.8       , 0.92307692])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAN_dwn_time_peak :  0.22569869867018\n",
      "IMAN_dwn_num_peak :  0.17516786569384635\n",
      "dwl_bytes_std :  0.15965810104277414\n",
      "upl_bytes_avg :  0.12010014165029204\n",
      "upl_bytes_std :  0.11799720850901319\n",
      "upl_max_psd :  0.0790332939856815\n",
      "dwl_peak_prom :  0.0755371137334339\n",
      "dwl_peak_std :  0.04680757671477871\n",
      "dwl_peak_avg :  0.0\n",
      "dwl_max_psd :  0.0\n",
      "dwl_bytes_avg :  0.0\n",
      "upl_peak_freq :  0.0\n",
      "dwl_peak_var :  0.0\n",
      "upl_peak_prom :  0.0\n",
      "upl_peak_avg :  0.0\n",
      "upl_peak_var :  0.0\n",
      "upl_peak_std :  0.0\n",
      "dwl_peak_freq :  0.0\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "  'dwl_peak_freq',\n",
    "  'dwl_peak_prom',\n",
    "  'dwl_max_psd',\n",
    "  'dwl_bytes_avg',\n",
    "  'dwl_bytes_std',\n",
    "  'dwl_peak_avg',\n",
    "  'dwl_peak_var',\n",
    "  'dwl_peak_std',\n",
    "  'upl_peak_freq',\n",
    "  'upl_peak_prom',\n",
    "  'upl_max_psd',\n",
    "  'upl_bytes_avg',\n",
    "  'upl_bytes_std',\n",
    "  'upl_peak_avg',\n",
    "  'upl_peak_var',\n",
    "  'upl_peak_std',\n",
    "  'IMAN_dwn_time_peak',#'IMAN_up_time_peak'\n",
    "            'IMAN_dwn_num_peak']#,'IMAN_up_num_peak']\n",
    "importances = clf_split.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(clf_split, open(\"randomforest_chkpt2.obj\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dill.load(open(\"randomforest_chkpt2.obj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no split (download focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)\n",
    "med_feat_no_split = create_features_no_split(stdoan_med, 100)\n",
    "high_feat_no_split = create_features_no_split(stdoan_high, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat_no_split['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat_no_split['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_no_split = pd.concat([low_feat_no_split, med_feat_no_split, high_feat_no_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_no_split.drop(columns=['resolution']), training_no_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_no_split = RandomForestClassifier(n_estimators = 2, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_no_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_no_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(training_no_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
