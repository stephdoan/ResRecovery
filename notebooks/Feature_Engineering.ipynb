{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbit_rate = 1/125000\n",
    "\n",
    "two_four_fp = '../data/240p/'\n",
    "three_six_fp = '../data/360p/'\n",
    "four_eight_fp = '../data/480p/'\n",
    "seven_two_fp = '../data/720p/'\n",
    "ten_eight_fp = '../data/1080p/'\n",
    "\n",
    "two_four_dir = os.listdir(two_four_fp)\n",
    "three_six_dir = os.listdir(three_six_fp)\n",
    "four_eight_dir = os.listdir(four_eight_fp)\n",
    "seven_two_dir = os.listdir(seven_two_fp)\n",
    "ten_eight_dir = os.listdir(ten_eight_fp)\n",
    "\n",
    "df_240_lst = [add_resolution(two_four_fp + fp, '240p') for fp in two_four_dir]\n",
    "df_360_lst = [add_resolution(three_six_fp + fp, '360p') for fp in three_six_dir]\n",
    "df_480_lst = [add_resolution(four_eight_fp + fp, '480p') for fp in four_eight_dir]\n",
    "df_720_lst = [add_resolution(seven_two_fp + fp, '720p') for fp in seven_two_dir]\n",
    "df_1080_lst = [add_resolution(ten_eight_fp + fp, '1080p') for fp in ten_eight_dir]\n",
    "\n",
    "chunk_240_lst = sum([chunk_data(df) for df in df_240_lst], [])\n",
    "chunk_360_lst = sum([chunk_data(df) for df in df_360_lst], [])\n",
    "chunk_480_lst = sum([chunk_data(df) for df in df_480_lst], [])\n",
    "chunk_720_lst = sum([chunk_data(df) for df in df_720_lst], [])\n",
    "chunk_1080_lst = sum([chunk_data(df) for df in df_1080_lst], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features\n",
    "In our EDA, we saw significant differences in aggregate statistics such as the mean and standard deviation. We reconfirm this by taking our chunked data and performing said operations. There seems to be alot of potential colinearity between the bytes and packet stream statistics (strong positive correlation). In our model, we chose to take the aggregate features of just the download stream of bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_features_all(df_lst):\n",
    "  download_byte_feat = [np.array([np.mean(df['2->1Bytes']), np.std(df['2->1Bytes'])]) * mbit_rate for df in df_lst]\n",
    "  download_pkt_feat = [np.array([np.mean(df['2->1Pkts']), np.std(df['2->1Pkts'])]) for df in df_lst]\n",
    "  upload_byte_feat = [np.array([np.mean(df['1->2Bytes']), np.std(df['1->2Bytes'])]) * mbit_rate for df in df_lst]\n",
    "  upload_pkt_feat = [np.array([np.mean(df['1->2Pkts']), np.std(df['1->2Pkts'])]) for df in df_lst]\n",
    "  return [download_byte_feat, download_pkt_feat, upload_byte_feat, upload_pkt_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68929318e-01, 1.15340577e+00],\n",
       "       [2.68640812e+01, 1.09422755e+02],\n",
       "       [2.01655931e-02, 6.47301899e-02],\n",
       "       [1.77151682e+01, 6.06875203e+01]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agg_features_all(chunk_240_lst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.47693444e-01, 1.62085922e+00],\n",
       "       [3.32106015e+01, 1.50414272e+02],\n",
       "       [2.10986046e-02, 8.15594338e-02],\n",
       "       [1.89911651e+01, 7.70355700e+01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agg_features_all(chunk_360_lst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.47737180e-01, 2.45569632e+00],\n",
       "       [6.28800533e+01, 2.31008800e+02],\n",
       "       [4.12340273e-02, 1.27121836e-01],\n",
       "       [3.73126416e+01, 1.23173373e+02]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agg_features_all(chunk_480_lst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32797336e+00, 3.51618586e+00],\n",
       "       [1.24230574e+02, 3.25938092e+02],\n",
       "       [6.96803972e-02, 1.69834253e-01],\n",
       "       [6.61051448e+01, 1.65536005e+02]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agg_features_all(chunk_720_lst), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.95912153e+00, 4.25834585e+00],\n",
       "       [1.89135641e+02, 4.00306507e+02],\n",
       "       [1.06824435e-01, 2.15001592e-01],\n",
       "       [1.05286826e+02, 2.08072070e+02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agg_features_all(chunk_1080_lst), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Related Aggregate Features\n",
    "Peaks were a strong point of focus in our EDA as there might be strong potential features to be extracted. We start by taking the definition of a \"peak\" from before: 1 standard deviation above the mean. The 1 factor is a heuristic we landed upon after experimentation. Knowing how many spikes is very useful but also knowing the time delay between each spikes is useful. We will also apply a signal processing technique of evenly spacing out spikes and then taking the time delay between them. We also experiement with a hard threshold of 5 Mbps as a peak. We realize that this will not be applicable to a wide variety of other users as people's internet speed can wildly vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_loc(df, col, invert=False):\n",
    "  'invert arg allows you to get values not considered peaks'\n",
    "  df_avg = df[col].mean()\n",
    "  df_std = df[col].std()\n",
    "  \n",
    "  threshold = df_avg + (1 * df_std)\n",
    "  if invert:\n",
    "    return np.array(df[col] < threshold)\n",
    "  \n",
    "  else:\n",
    "    return np.array(df[col] > threshold)\n",
    "  \n",
    "def hard_threshold_peaks(df, col, thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = sp.signal.find_peaks(x, height=thresh)\n",
    "    return peaks\n",
    "  \n",
    "def peak_time_diff(df, col):\n",
    "  '''\n",
    "  mess around with the different inputs for function. \n",
    "  variance seems to inflate the difference betweent the two the most with litte\n",
    "  to no data manipulation. however, currently trying things like\n",
    "  squaring the data before taking the aggregate function to exaggerate\n",
    "  differences (moderate success??)\n",
    "  '''\n",
    "  #peaks = df[get_peak_loc(df, col)]\n",
    "  peaks = df.iloc[hard_threshold_peaks(df, col, 625000)]\n",
    "  peaks['Time'] = peaks['Time'] - peaks['Time'].min()\n",
    "  time_diff = np.diff(peaks['Time'])\n",
    "  if len(peaks) <= 0:\n",
    "    return [-1, 0, -1]\n",
    "  \n",
    "  return [np.mean(peaks)['2->1Bytes'] * mbit_rate, len(peaks), 120 / len(peaks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "peaks_240 = [peak_time_diff(df, '2->1Bytes') for df in chunk_240_lst]\n",
    "peaks_360 = [peak_time_diff(df, '2->1Bytes') for df in chunk_360_lst]\n",
    "peaks_480 = [peak_time_diff(df, '2->1Bytes') for df in chunk_480_lst]\n",
    "peaks_720 = [peak_time_diff(df, '2->1Bytes') for df in chunk_720_lst]\n",
    "peaks_1080 = [peak_time_diff(df, '2->1Bytes') for df in chunk_1080_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.8263235 ,  3.09638554, 48.36890419])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(peaks_240, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.3893675 ,  3.46987952, 40.3350545 ])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(peaks_360, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.35970354,  4.68674699, 32.56751682])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(peaks_480, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.09373498,  9.32926829, 19.91668919])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.nan_to_num(peaks_720), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.26072742, 15.24096386, 11.27606313])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(peaks_1080, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_features(df, col):\n",
    "    \"\"\"\n",
    "    welch implemention of spectral features\n",
    "    resample the data before inputting (might change prereq depending on\n",
    "    resource allocation)\n",
    "    \"\"\"\n",
    "\n",
    "    f, Pxx_den = sp.signal.welch(df[col], fs=2)\n",
    "    Pxx_den = np.sqrt(Pxx_den)\n",
    "\n",
    "    peaks = sp.signal.find_peaks(Pxx_den)[0]\n",
    "    prominences = sp.signal.peak_prominences(Pxx_den, peaks)[0]\n",
    "\n",
    "    idx_max = prominences.argmax()\n",
    "    loc_max = peaks[idx_max]\n",
    "\n",
    "    return [f[loc_max], Pxx_den[loc_max], prominences[idx_max]]\n",
    "  \n",
    "def power_density(df, bins):\n",
    "  \n",
    "  f_temp, Pxx_temp = sp.signal.welch(df['pkt_size'], fs=.5) \n",
    "  Pxx_temp = np.sqrt(Pxx_temp)\n",
    "  freq = np.linspace(0, np.max(f_temp) + .01, num=bins) - .001\n",
    "  total = np.trapz(y=Pxx_temp, x=f_temp)\n",
    "  temp_lst = []\n",
    "  \n",
    "  for i in np.arange(len(freq) - 1):\n",
    "\n",
    "    f_lower = np.where(f_temp >= freq[i])\n",
    "    f_upper = np.where(f_temp < freq[i+1] )\n",
    "    selected_range = np.intersect1d(f_lower, f_upper)\n",
    "\n",
    "    pxx_den = np.trapz(y=Pxx_temp[selected_range], x=f_temp[selected_range]) / total\n",
    "    temp_lst.append(pxx_den)\n",
    "\n",
    "  return temp_lst\n",
    "\n",
    "def temp_name(df):\n",
    "  f_temp, Pxx_temp = sp.signal.welch(df['pkt_size'], fs=2) \n",
    "  return np.sum((Pxx_temp/np.sum(Pxx_temp)) * f_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "size='500ms'\n",
    "resample_240 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_240_lst]\n",
    "resample_360 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_360_lst]\n",
    "resample_480 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_480_lst]\n",
    "resample_720 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_720_lst]\n",
    "resample_1080 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_1080_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "size='2000ms'\n",
    "\n",
    "resample_240 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_240_lst]\n",
    "resample_360 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_360_lst]\n",
    "resample_480 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_480_lst]\n",
    "resample_720 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_720_lst]\n",
    "resample_1080 = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_1080_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxx_den_240 = [power_density(df, 4) for df in resample_240]\n",
    "pxx_den_360 = [power_density(df, 4) for df in resample_360]\n",
    "pxx_den_480 = [power_density(df, 4) for df in resample_480]\n",
    "pxx_den_720 = [power_density(df, 4) for df in resample_720]\n",
    "pxx_den_1080 = [power_density(df, 4) for df in resample_1080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>[-.001, .086)</th>\n",
       "      <th>[.086, .172)</th>\n",
       "      <th>[.172, .259)</th>\n",
       "      <th>St_dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>240p</th>\n",
       "      <td>0.309454</td>\n",
       "      <td>0.325631</td>\n",
       "      <td>0.297976</td>\n",
       "      <td>0.011345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360p</th>\n",
       "      <td>0.318108</td>\n",
       "      <td>0.316864</td>\n",
       "      <td>0.296115</td>\n",
       "      <td>0.010087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480p</th>\n",
       "      <td>0.316858</td>\n",
       "      <td>0.328683</td>\n",
       "      <td>0.284150</td>\n",
       "      <td>0.018835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720p</th>\n",
       "      <td>0.257639</td>\n",
       "      <td>0.342087</td>\n",
       "      <td>0.333840</td>\n",
       "      <td>0.038015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080p</th>\n",
       "      <td>0.223995</td>\n",
       "      <td>0.330782</td>\n",
       "      <td>0.380637</td>\n",
       "      <td>0.065342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       [-.001, .086)  [.086, .172)  [.172, .259)    St_dev\n",
       "240p        0.309454      0.325631      0.297976  0.011345\n",
       "360p        0.318108      0.316864      0.296115  0.010087\n",
       "480p        0.316858      0.328683      0.284150  0.018835\n",
       "720p        0.257639      0.342087      0.333840  0.038015\n",
       "1080p       0.223995      0.330782      0.380637  0.065342"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxx_den_df = pd.DataFrame({\n",
    "  \"240p\": np.mean(pxx_den_240, axis=0),\n",
    "  \"360p\": np.mean(pxx_den_360, axis=0),\n",
    "  \"480p\": np.mean(pxx_den_480, axis=0),\n",
    "  \"720p\": np.mean(pxx_den_720, axis=0),\n",
    "  \"1080p\": np.mean(pxx_den_1080, axis=0)\n",
    "}).T\n",
    "\n",
    "pxx_den_df.columns = ['[-.001, .086)', '[.086, .172)', '[.172, .259)']\n",
    "pxx_den_df['St_dev'] = [\n",
    "  np.std(np.mean(pxx_den_240, axis=0)),\n",
    "  np.std(np.mean(pxx_den_360, axis=0)),\n",
    "  np.std(np.mean(pxx_den_480, axis=0)),\n",
    "  np.std(np.mean(pxx_den_720, axis=0)), \n",
    "  np.std(np.mean(pxx_den_1080, axis=0))]\n",
    "\n",
    "pxx_den_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.001     ,  0.08566667,  0.17233333,  0.259     ])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0, .25 + .01, num=4) - .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31810831, 0.31686423, 0.29611528])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(pxx_den_360, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31685758, 0.32868302, 0.28414961])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(pxx_den_480, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25763884, 0.34208657, 0.3338403 ])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(pxx_den_720, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22399455, 0.33078182, 0.38063741])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.mean(pxx_den_1080, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wip; need to decide chunk size eventually\n",
    "## should we also make this chunking feature be our feature creation?\n",
    "\n",
    "def chunk_data(df, interval=60):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in a filepath to the data you want to chunk and feature engineer\n",
    "    chunks our data into a specified time interval\n",
    "    each chunk is then turned into an observation to be fed into our classifier\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    df['Time'] = df['Time'] - df['Time'].min()\n",
    "    \n",
    "    total_chunks = np.floor(df['Time'].max() / interval).astype(int)\n",
    "\n",
    "    for chunk in np.arange(total_chunks):\n",
    "      \n",
    "        start = chunk * interval\n",
    "        end = (chunk+1) * interval\n",
    "\n",
    "        temp_df = (df[(df['Time'] >= start) & (df['Time'] < end)])\n",
    "        \n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'dwl_peak_freq',\n",
    "    'dwl_peak_prom',\n",
    "    'dwl_max_psd',\n",
    "    'dwl_bytes_avg',\n",
    "    'dwl_bytes_std',\n",
    "    'dwl_peak_avg',\n",
    "    'dwl_peak_var',\n",
    "    'dwl_peak_std',\n",
    "    'upl_peak_freq',\n",
    "    'upl_peak_prom',\n",
    "    'upl_max_psd',\n",
    "    'upl_bytes_avg',\n",
    "    'upl_bytes_std',\n",
    "    'upl_peak_avg',\n",
    "    'upl_peak_var',\n",
    "    'upl_peak_std',\n",
    "    'IMAN_dwn_time_peak',#'IMAN_up_time_peak',\n",
    "    'IMAN_dwn_num_peak'#,'IMAN_up_num_peak'\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "    upl_bytes = preproc[preproc['pkt_src'] == '1'].resample('500ms', on='Time').sum()\n",
    "    dwl_bytes = preproc[preproc['pkt_src'] == '2'].resample('500ms', on='Time').sum()\n",
    "\n",
    "    ## spectral features\n",
    "    dwl_spectral = spectral_features(dwl_bytes, 'pkt_size')\n",
    "    upl_spectral = spectral_features(upl_bytes, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    dwl_agg = agg_feat(chunk, '2->1Bytes')\n",
    "    upl_agg = agg_feat(chunk, '1->2Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    dwl_peak = peak_time_diff(chunk, '2->1Bytes')\n",
    "    upl_peak = peak_time_diff(chunk, '1->2Bytes')\n",
    "    \n",
    "    ## iman's time between peak \n",
    "    iman_dwn_time_peak = np.mean(peak_times(chunk,'2->1Bytes',1000000))\n",
    "    #iman_up_time_peak = np.mean(peak_times(chunk,'1->2Bytes',50000))\n",
    "\n",
    "    ## iman's num peak\n",
    "    iman_dwn_num_peak = num_peaks(chunk,'2->1Bytes',1000000)\n",
    "    #iman_up_num_peak = num_peaks(chunk,'1->2Bytes',50000)\n",
    "\n",
    "    feat_val = np.hstack((\n",
    "      dwl_spectral,\n",
    "      dwl_agg,\n",
    "      dwl_peak,\n",
    "      upl_spectral,\n",
    "      upl_agg,\n",
    "      upl_peak,\n",
    "      iman_dwn_time_peak,\n",
    "      #iman_up_time_peak,\n",
    "      iman_dwn_num_peak,\n",
    "      #iman_up_num_peak\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_no_split(df, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'peak_freq',\n",
    "    'peak_prom',\n",
    "    'max_psd',\n",
    "    'bytes_avg',\n",
    "    'bytes_std',\n",
    "    'peak_avg',\n",
    "    'peak_var',\n",
    "    'peak_std',\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "\n",
    "  df_chunks = chunk_data(df, interval)\n",
    "\n",
    "  for chunk in df_chunks:\n",
    "\n",
    "    preproc = convert_ms_df(chunk, True)\n",
    "\n",
    "    ## spectral features\n",
    "    spectral_feat = spectral_features(preproc, 'pkt_size')\n",
    "    \n",
    "    ## aggregate features\n",
    "    aggr_feat = agg_feat(chunk, '2->1Bytes')\n",
    "    \n",
    "    ## peak features\n",
    "    peak_feat = peak_time_diff(chunk, '2->1Bytes')\n",
    "    \n",
    "    feat_val = np.hstack((\n",
    "      spectral_feat,\n",
    "      aggr_feat,\n",
    "      peak_feat\n",
    "    ))\n",
    "    \n",
    "    vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "low_feat = pd.concat([create_features(df, 100) for df in low_dfs])\n",
    "med_feat = pd.concat([create_features(df, 100) for df in med_dfs])\n",
    "high_feat = pd.concat([create_features(df, 100) for df in high_dfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = pd.concat([low_feat, med_feat, high_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_split.drop(columns=['resolution']), training_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_split = RandomForestClassifier(n_estimators = 5, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(training_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "  'dwl_peak_freq',\n",
    "  'dwl_peak_prom',\n",
    "  'dwl_max_psd',\n",
    "  'dwl_bytes_avg',\n",
    "  'dwl_bytes_std',\n",
    "  'dwl_peak_avg',\n",
    "  'dwl_peak_var',\n",
    "  'dwl_peak_std',\n",
    "  'upl_peak_freq',\n",
    "  'upl_peak_prom',\n",
    "  'upl_max_psd',\n",
    "  'upl_bytes_avg',\n",
    "  'upl_bytes_std',\n",
    "  'upl_peak_avg',\n",
    "  'upl_peak_var',\n",
    "  'upl_peak_std',\n",
    "  'IMAN_dwn_time_peak',#'IMAN_up_time_peak'\n",
    "            'IMAN_dwn_num_peak']#,'IMAN_up_num_peak']\n",
    "importances = clf_split.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(clf_split, open(\"randomforest_chkpt2.obj\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dill.load(open(\"randomforest_chkpt2.obj\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no split (download focus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "low_feat_no_split = create_features_no_split(stdoan_low, 100)\n",
    "med_feat_no_split = create_features_no_split(stdoan_med, 100)\n",
    "high_feat_no_split = create_features_no_split(stdoan_high, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat_no_split['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat_no_split['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat_no_split['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_no_split = pd.concat([low_feat_no_split, med_feat_no_split, high_feat_no_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_no_split.drop(columns=['resolution']), training_no_split['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_no_split = RandomForestClassifier(n_estimators = 2, max_depth = 2, criterion = 'entropy', random_state = 42)\n",
    "clf_no_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_no_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(training_no_split.corr()['resolution']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Resolution'], colnames=['Predicted Resolution']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
