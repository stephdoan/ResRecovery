{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')\n",
    "from features import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(20,5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbit_rate = 1/125000\n",
    "\n",
    "low_fp = '../data/240p/' \n",
    "threesixty_fp = '../data/360p/' \n",
    "med_fp = '../data/480p/'\n",
    "seventwenty_fp = '../data/720p/' \n",
    "high_fp = '../data/1080p/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dfs = []\n",
    "for file in os.listdir(low_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        low_dfs.append(pd.read_csv(low_fp+file))\n",
    "    \n",
    "threesixty_dfs = []\n",
    "for file in os.listdir(threesixty_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        threesixty_dfs.append(pd.read_csv(threesixty_fp+file))\n",
    "        \n",
    "med_dfs = []\n",
    "for file in os.listdir(med_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        med_dfs.append(pd.read_csv(med_fp+file))\n",
    "        \n",
    "seventwenty_dfs = []\n",
    "for file in os.listdir(seventwenty_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        seventwenty_dfs.append(pd.read_csv(seventwenty_fp+file))\n",
    "    \n",
    "high_dfs = []\n",
    "for file in os.listdir(high_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        high_dfs.append(pd.read_csv(high_fp+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdoan_low = pd.read_csv(low_fp + 'stdoan-101-action-240p-20201127.csv')\n",
    "# stdoan_med = pd.read_csv(med_fp + 'stdoan-101-action-480p-20201127.csv')\n",
    "# stdoan_high = pd.read_csv(high_fp + 'stdoan-101-action-1080p-20201127.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_ms = []\n",
    "for df in low_dfs:\n",
    "    low_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "threesixty_ms = []\n",
    "for df in threesixty_dfs:\n",
    "    threesixty_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "med_ms = []\n",
    "for df in med_dfs:\n",
    "    med_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "seventwenty_ms = []\n",
    "for df in seventwenty_dfs:\n",
    "    seventwenty_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "high_ms = []\n",
    "for df in high_dfs:\n",
    "    high_ms.append(convert_ms_df(df,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_ms = convert_ms_df(stdoan_low, True)\n",
    "# med_ms = convert_ms_df(stdoan_med, True)\n",
    "# high_ms = convert_ms_df(stdoan_high, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_resamples = []\n",
    "for df in low_ms:\n",
    "    low_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "threesixty_resamples = []\n",
    "for df in threesixty_ms:\n",
    "    threesixty_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "med_resamples = []\n",
    "for df in med_ms:\n",
    "    med_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "seventwenty_resamples = []\n",
    "for df in seventwenty_ms:\n",
    "    seventwenty_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "high_resamples = []\n",
    "for df in high_ms:\n",
    "    high_resamples.append(df.resample('500ms', on='Time').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_resample = low_ms.resample('500ms', on='Time').sum()\n",
    "# med_resample = med_ms.resample('500ms', on='Time').sum()\n",
    "# high_resample = high_ms.resample('500ms', on='Time').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the aggregate features of the whole chunk; download and upload\n",
    "def agg_feat(df, col):\n",
    "    return [np.mean(df[col]), np.std(df[col])]\n",
    "\n",
    "## take the ratio of upload:download packets\n",
    "def pkt_ratio(df):\n",
    "    ms_df = convert_ms_df(df, True)\n",
    "    local = np.sum(ms_df['pkt_src'] == '1') \n",
    "    server = np.sum(ms_df['pkt_src'] == '2') \n",
    "    return local / server\n",
    "\n",
    "## take the ratio of upload:download bytes\n",
    "def bytes_ratio(df):\n",
    "    local = df['1->2Bytes'].sum()\n",
    "    server = df['2->1Bytes'].sum()\n",
    "    return local / server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Related Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finds the peaks with mean + 2(1) std\n",
    "## run the above aggregate functions on the peaks only??\n",
    "\n",
    "def get_peak_loc(df, col, invert=False):\n",
    "  'invert arg allows you to get values not considered peaks'\n",
    "  df_avg = df[col].mean()\n",
    "  df_std = df[col].std()\n",
    "  \n",
    "  threshold = df_avg + (1 * df_std)\n",
    "  \n",
    "  if invert:\n",
    "    return np.array(df[col] < threshold)\n",
    "  \n",
    "  else:\n",
    "    return np.array(df[col] > threshold)\n",
    "\n",
    "## np.mean, np.var, np.std - think of more?  \n",
    "def peak_time_diff(df, col):\n",
    "  '''\n",
    "  mess around with the different inputs for function. \n",
    "  variance seems to inflate the difference betweent the two the most with litte\n",
    "  to no data manipulation. however, currently trying things like\n",
    "  squaring the data before taking the aggregate function to exaggerate\n",
    "  differences (moderate success??)\n",
    "  '''\n",
    "  peaks = df[get_peak_loc(df, col)]\n",
    "  peaks['Time'] = peaks['Time'] - peaks['Time'].min()\n",
    "  time_diff = np.diff(peaks['Time'] ** 2)\n",
    "  return [np.mean(time_diff), np.std(time_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def peak_times(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    if list(peaks) == []:\n",
    "        return [-1]\n",
    "    times = df.iloc[peaks]['Time'].values\n",
    "    time_between_peaks = [times[i]-times[i-1]for i in range(1,len(times))]\n",
    "    #print(time_between_peaks)\n",
    "    #time_between_peaks[0]=0\n",
    "    if time_between_peaks == []:\n",
    "        return -1\n",
    "    return time_between_peaks\n",
    "\n",
    "def num_peaks(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    return len(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_features(df, col):\n",
    "\n",
    "    \"\"\"\n",
    "    welch implemention of spectral features\n",
    "    resample the data before inputting (might change prereq depending on\n",
    "    resource allocation)\n",
    "    \"\"\"\n",
    "\n",
    "    f, Pxx_den = sp.signal.welch(df[col], fs=2)\n",
    "    Pxx_den = np.sqrt(Pxx_den)\n",
    "\n",
    "    peaks = sp.signal.find_peaks(Pxx_den)[0]\n",
    "    prominences = sp.signal.peak_prominences(Pxx_den, peaks)[0]\n",
    "\n",
    "    idx_max = prominences.argmax()\n",
    "    loc_max = peaks[idx_max]\n",
    "\n",
    "    return [f[loc_max], Pxx_den[loc_max], prominences[idx_max]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wip; need to decide chunk size eventually\n",
    "## should we also make this chunking feature be our feature creation?\n",
    "\n",
    "def chunk_data(df, interval=60):\n",
    "\n",
    "    \"\"\"\n",
    "    takes in a filepath to the data you want to chunk and feature engineer\n",
    "    chunks our data into a specified time interval\n",
    "    each chunk is then turned into an observation to be fed into our classifier\n",
    "    \"\"\"\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    df['Time'] = df['Time'] - df['Time'].min()\n",
    "    \n",
    "    total_chunks = np.floor(df['Time'].max() / interval).astype(int)\n",
    "\n",
    "    for chunk in np.arange(total_chunks):\n",
    "      \n",
    "        start = chunk * interval\n",
    "        end = (chunk+1) * interval\n",
    "\n",
    "        temp_df = (df[(df['Time'] >= start) & (df['Time'] < end)])\n",
    "        \n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(dfs, interval=60):\n",
    "\n",
    "  features = [\n",
    "    'dwl_peak_freq',\n",
    "    'dwl_peak_prom',\n",
    "    'dwl_max_psd',\n",
    "    'dwl_bytes_avg',\n",
    "    'dwl_bytes_std',\n",
    "    'dwl_peak_avg',\n",
    "    'dwl_peak_std',\n",
    "    'upl_peak_freq',\n",
    "    'upl_peak_prom',\n",
    "    'upl_max_psd',\n",
    "    'upl_bytes_avg',\n",
    "    'upl_bytes_std',\n",
    "    'upl_peak_avg',\n",
    "    'upl_peak_std',\n",
    "    'dwl_time_peak',#'IMAN_up_time_peak',\n",
    "      'dwl_num_peak'#,'IMAN_up_num_peak'\n",
    "  ]  \n",
    "\n",
    "  vals = []\n",
    "  for df in dfs:\n",
    "      df_chunks = chunk_data(df, interval)\n",
    "\n",
    "      for chunk in df_chunks:\n",
    "\n",
    "        preproc = convert_ms_df(chunk, True)\n",
    "        upl_bytes = preproc[preproc['pkt_src'] == '1'].resample('500ms', on='Time').sum()\n",
    "        dwl_bytes = preproc[preproc['pkt_src'] == '2'].resample('500ms', on='Time').sum()\n",
    "\n",
    "        ## spectral features\n",
    "        dwl_spectral = spectral_features(dwl_bytes, 'pkt_size')\n",
    "        upl_spectral = spectral_features(upl_bytes, 'pkt_size')\n",
    "\n",
    "        ## aggregate features\n",
    "        dwl_agg = agg_feat(chunk, '2->1Bytes')\n",
    "        upl_agg = agg_feat(chunk, '1->2Bytes')\n",
    "\n",
    "        ## peak features\n",
    "        dwl_peak = peak_time_diff(chunk, '2->1Bytes')\n",
    "        upl_peak = peak_time_diff(chunk, '1->2Bytes')\n",
    "        \n",
    "        ## iman's time between peak \n",
    "        iman_dwn_time_peak = np.mean(peak_times(chunk,'2->1Bytes',1000000))\n",
    "        #iman_up_time_peak = np.mean(peak_times(chunk,'1->2Bytes',50000))\n",
    "        \n",
    "        ## iman's num peak\n",
    "        iman_dwn_num_peak = num_peaks(chunk,'2->1Bytes',1000000)\n",
    "        #iman_up_num_peak = num_peaks(chunk,'1->2Bytes',50000)\n",
    "\n",
    "\n",
    "        \n",
    "        feat_val = np.hstack((\n",
    "          dwl_spectral,\n",
    "          dwl_agg,\n",
    "          dwl_peak,\n",
    "          upl_spectral,\n",
    "          upl_agg,\n",
    "          upl_peak,\n",
    "            iman_dwn_time_peak,#iman_up_time_peak,\n",
    "            iman_dwn_num_peak,#iman_up_num_peak\n",
    "        ))\n",
    "\n",
    "        vals.append(feat_val)\n",
    "    \n",
    "  return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Model (low/med/high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 737 ms, total: 1min 15s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "low_feat = create_features(low_dfs, chunk_size)\n",
    "threesixty_feat = create_features(threesixty_dfs, chunk_size)\n",
    "med_feat = create_features(med_dfs, chunk_size)\n",
    "seventwenty_feat = create_features(seventwenty_dfs, chunk_size)\n",
    "high_feat = create_features(high_dfs, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat['resolution'] = 1\n",
    "threesixty_feat['resolution'] = 1\n",
    "med_feat['resolution'] = 2\n",
    "seventwenty_feat['resolution'] = 3\n",
    "high_feat['resolution'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat([low_feat, threesixty_feat, med_feat,seventwenty_feat, high_feat]).reset_index(drop=True)\n",
    "#training = pd.concat([low_feat, med_feat, high_feat]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT SUBSETS OF FEATURES\n",
    "\n",
    "#training = training[['dwl_bytes_avg','dwl_peak_prom','upl_bytes_std','dwl_bytes_std','upl_peak_std','resolution']]\n",
    "#training = training[['dwl_bytes_avg','upl_max_psd','dwl_max_psd','upl_peak_prom','dwl_num_peak','dwl_peak_prom','resolution']]\n",
    "#training = training[['dwl_max_psd','upl_max_psd','dwl_peak_prom','upl_peak_prom','dwl_num_peak','dwl_bytes_avg','upl_bytes_std','upl_bytes_avg','resolution']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dwl_peak_freq</th>\n",
       "      <th>dwl_peak_prom</th>\n",
       "      <th>dwl_max_psd</th>\n",
       "      <th>dwl_bytes_avg</th>\n",
       "      <th>dwl_bytes_std</th>\n",
       "      <th>dwl_peak_avg</th>\n",
       "      <th>dwl_peak_std</th>\n",
       "      <th>upl_peak_freq</th>\n",
       "      <th>upl_peak_prom</th>\n",
       "      <th>upl_max_psd</th>\n",
       "      <th>upl_bytes_avg</th>\n",
       "      <th>upl_bytes_std</th>\n",
       "      <th>upl_peak_avg</th>\n",
       "      <th>upl_peak_std</th>\n",
       "      <th>dwl_time_peak</th>\n",
       "      <th>dwl_num_peak</th>\n",
       "      <th>resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>2.067111e+05</td>\n",
       "      <td>1.575466e+05</td>\n",
       "      <td>74919.477178</td>\n",
       "      <td>234944.058940</td>\n",
       "      <td>3313.500000</td>\n",
       "      <td>3626.211338</td>\n",
       "      <td>0.398438</td>\n",
       "      <td>10126.192987</td>\n",
       "      <td>7042.031568</td>\n",
       "      <td>5038.771784</td>\n",
       "      <td>13131.996598</td>\n",
       "      <td>3180.960000</td>\n",
       "      <td>3611.240845</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>1.695320e+05</td>\n",
       "      <td>1.190181e+05</td>\n",
       "      <td>52953.709184</td>\n",
       "      <td>189342.162055</td>\n",
       "      <td>5207.142857</td>\n",
       "      <td>4297.348233</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>9795.036377</td>\n",
       "      <td>7680.135744</td>\n",
       "      <td>3749.346939</td>\n",
       "      <td>9452.349820</td>\n",
       "      <td>5207.142857</td>\n",
       "      <td>4297.348233</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>1.458635e+05</td>\n",
       "      <td>1.273509e+05</td>\n",
       "      <td>53921.263441</td>\n",
       "      <td>187013.108307</td>\n",
       "      <td>5168.642857</td>\n",
       "      <td>4197.561139</td>\n",
       "      <td>0.101562</td>\n",
       "      <td>9209.022157</td>\n",
       "      <td>7792.282403</td>\n",
       "      <td>3889.161290</td>\n",
       "      <td>9249.333985</td>\n",
       "      <td>5168.642857</td>\n",
       "      <td>4197.561139</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>1.582415e+05</td>\n",
       "      <td>1.249173e+05</td>\n",
       "      <td>54971.453202</td>\n",
       "      <td>177587.670312</td>\n",
       "      <td>4389.062500</td>\n",
       "      <td>5208.934614</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>8731.614547</td>\n",
       "      <td>5625.941138</td>\n",
       "      <td>4382.896552</td>\n",
       "      <td>9771.021738</td>\n",
       "      <td>4009.263158</td>\n",
       "      <td>5296.348433</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.183220e+05</td>\n",
       "      <td>1.934314e+05</td>\n",
       "      <td>61085.956522</td>\n",
       "      <td>184739.231965</td>\n",
       "      <td>4513.470588</td>\n",
       "      <td>4773.580591</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>10758.495890</td>\n",
       "      <td>9412.021724</td>\n",
       "      <td>4099.266304</td>\n",
       "      <td>9195.293407</td>\n",
       "      <td>4262.722222</td>\n",
       "      <td>4752.317117</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.132210e+06</td>\n",
       "      <td>1.067360e+06</td>\n",
       "      <td>272979.757848</td>\n",
       "      <td>607287.699478</td>\n",
       "      <td>3024.321429</td>\n",
       "      <td>2261.033251</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>54511.415466</td>\n",
       "      <td>51763.803227</td>\n",
       "      <td>14006.955157</td>\n",
       "      <td>28828.563426</td>\n",
       "      <td>2920.034483</td>\n",
       "      <td>2245.368897</td>\n",
       "      <td>10.769231</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>1.172365e+06</td>\n",
       "      <td>1.107221e+06</td>\n",
       "      <td>283183.631579</td>\n",
       "      <td>643340.966516</td>\n",
       "      <td>2712.903226</td>\n",
       "      <td>1970.125759</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>56393.386537</td>\n",
       "      <td>53164.228734</td>\n",
       "      <td>14486.934211</td>\n",
       "      <td>30594.990566</td>\n",
       "      <td>2712.903226</td>\n",
       "      <td>1970.125759</td>\n",
       "      <td>10.035714</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>1.126711e+06</td>\n",
       "      <td>1.037700e+06</td>\n",
       "      <td>329029.936073</td>\n",
       "      <td>666420.208304</td>\n",
       "      <td>2556.125000</td>\n",
       "      <td>1817.073226</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>54170.918310</td>\n",
       "      <td>50284.230909</td>\n",
       "      <td>16728.109589</td>\n",
       "      <td>31617.181082</td>\n",
       "      <td>2556.125000</td>\n",
       "      <td>1817.073226</td>\n",
       "      <td>9.862069</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>1.277462e+06</td>\n",
       "      <td>1.233146e+06</td>\n",
       "      <td>259038.224576</td>\n",
       "      <td>579092.410102</td>\n",
       "      <td>2574.031250</td>\n",
       "      <td>3048.064411</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>60714.980350</td>\n",
       "      <td>58427.532006</td>\n",
       "      <td>13478.610169</td>\n",
       "      <td>27556.855153</td>\n",
       "      <td>2657.064516</td>\n",
       "      <td>3061.068366</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>8.142050e+05</td>\n",
       "      <td>7.452549e+05</td>\n",
       "      <td>221399.924051</td>\n",
       "      <td>557575.600700</td>\n",
       "      <td>2761.689655</td>\n",
       "      <td>2326.296852</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>38989.388000</td>\n",
       "      <td>35552.959671</td>\n",
       "      <td>11495.421941</td>\n",
       "      <td>26569.728026</td>\n",
       "      <td>2761.689655</td>\n",
       "      <td>2326.296852</td>\n",
       "      <td>12.136364</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dwl_peak_freq  dwl_peak_prom   dwl_max_psd  dwl_bytes_avg  dwl_bytes_std  \\\n",
       "0         0.203125   2.067111e+05  1.575466e+05   74919.477178  234944.058940   \n",
       "1         0.835938   1.695320e+05  1.190181e+05   52953.709184  189342.162055   \n",
       "2         0.234375   1.458635e+05  1.273509e+05   53921.263441  187013.108307   \n",
       "3         0.023438   1.582415e+05  1.249173e+05   54971.453202  177587.670312   \n",
       "4         0.031250   2.183220e+05  1.934314e+05   61085.956522  184739.231965   \n",
       "..             ...            ...           ...            ...            ...   \n",
       "273       0.187500   1.132210e+06  1.067360e+06  272979.757848  607287.699478   \n",
       "274       0.187500   1.172365e+06  1.107221e+06  283183.631579  643340.966516   \n",
       "275       0.109375   1.126711e+06  1.037700e+06  329029.936073  666420.208304   \n",
       "276       0.093750   1.277462e+06  1.233146e+06  259038.224576  579092.410102   \n",
       "277       0.187500   8.142050e+05  7.452549e+05  221399.924051  557575.600700   \n",
       "\n",
       "     dwl_peak_avg  dwl_peak_std  upl_peak_freq  upl_peak_prom   upl_max_psd  \\\n",
       "0     3313.500000   3626.211338       0.398438   10126.192987   7042.031568   \n",
       "1     5207.142857   4297.348233       0.101562    9795.036377   7680.135744   \n",
       "2     5168.642857   4197.561139       0.101562    9209.022157   7792.282403   \n",
       "3     4389.062500   5208.934614       0.328125    8731.614547   5625.941138   \n",
       "4     4513.470588   4773.580591       0.031250   10758.495890   9412.021724   \n",
       "..            ...           ...            ...            ...           ...   \n",
       "273   3024.321429   2261.033251       0.187500   54511.415466  51763.803227   \n",
       "274   2712.903226   1970.125759       0.187500   56393.386537  53164.228734   \n",
       "275   2556.125000   1817.073226       0.109375   54170.918310  50284.230909   \n",
       "276   2574.031250   3048.064411       0.093750   60714.980350  58427.532006   \n",
       "277   2761.689655   2326.296852       0.187500   38989.388000  35552.959671   \n",
       "\n",
       "     upl_bytes_avg  upl_bytes_std  upl_peak_avg  upl_peak_std  dwl_time_peak  \\\n",
       "0      5038.771784   13131.996598   3180.960000   3611.240845      13.000000   \n",
       "1      3749.346939    9452.349820   5207.142857   4297.348233      -1.000000   \n",
       "2      3889.161290    9249.333985   5168.642857   4197.561139      -1.000000   \n",
       "3      4382.896552    9771.021738   4009.263158   5296.348433      -1.000000   \n",
       "4      4099.266304    9195.293407   4262.722222   4752.317117      -1.000000   \n",
       "..             ...            ...           ...           ...            ...   \n",
       "273   14006.955157   28828.563426   2920.034483   2245.368897      10.769231   \n",
       "274   14486.934211   30594.990566   2712.903226   1970.125759      10.035714   \n",
       "275   16728.109589   31617.181082   2556.125000   1817.073226       9.862069   \n",
       "276   13478.610169   27556.855153   2657.064516   3061.068366      10.200000   \n",
       "277   11495.421941   26569.728026   2761.689655   2326.296852      12.136364   \n",
       "\n",
       "     dwl_num_peak  resolution  \n",
       "0             2.0           1  \n",
       "1             1.0           1  \n",
       "2             1.0           1  \n",
       "3             0.0           1  \n",
       "4             0.0           1  \n",
       "..            ...         ...  \n",
       "273          27.0           3  \n",
       "274          29.0           3  \n",
       "275          30.0           3  \n",
       "276          26.0           3  \n",
       "277          23.0           3  \n",
       "\n",
       "[278 rows x 17 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training.drop(columns=['resolution']), training['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4,stratify=training['resolution'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\n",
    "#classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 42)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Group</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Group   1  2   3\n",
       "Actual Group              \n",
       "1                11  1   0\n",
       "2                 0  5   1\n",
       "3                 0  0  12"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN ON TEST DATA\n",
    "\n",
    "low_test = pd.read_csv('../data/test/sgs008-109-action-240p-20210202.csv')\n",
    "med_test = pd.read_csv('../data/test/sgs008-109-action-480p-20210202.csv')\n",
    "high_test = pd.read_csv('../data/test/sgs008-109-action-1080p-20210202.csv')\n",
    "threesixty_test = pd.read_csv('../data/test/sgs008-109-action-360p-20210213.csv')\n",
    "seventwenty_test = pd.read_csv('../data/test/sgs008-109-action-720p-20210213.csv')\n",
    "#low_test = pd.read_csv('../data/test/sgs008-109-action-360p-20210213.csv')\n",
    "#low_test = pd.read_csv('../data/test/stdoan-102-action-720p-20201206.csv')\n",
    "\n",
    "low_test_feat = create_features([low_test], chunk_size)\n",
    "threesixty_test_feat = create_features([threesixty_test], chunk_size)\n",
    "med_test_feat = create_features([med_test], chunk_size)\n",
    "seventwenty_test_feat = create_features([seventwenty_test], chunk_size)\n",
    "high_test_feat = create_features([high_test], chunk_size)\n",
    "\n",
    "low_test_feat['resolution'] = 1\n",
    "threesixty_test_feat['resolution'] = 1\n",
    "med_test_feat['resolution'] = 2\n",
    "seventwenty_test_feat['resolution'] = 3\n",
    "high_test_feat['resolution'] = 3\n",
    "\n",
    "test = pd.concat([low_test_feat, threesixty_test_feat, med_test_feat,seventwenty_test_feat, high_test_feat])\n",
    "\n",
    "classifier.fit(X,y)\n",
    "y_pred = classifier.predict(test.drop('resolution',axis=1))\n",
    "(pd.crosstab(test['resolution'], y_pred, rownames=['Actual Group'], colnames=['Predicted Group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['resolution'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"model.pkl\"\n",
    "with open(pkl_filename, 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test = create_features([pd.read_csv('../data/test/testme.csv')],chunk_size)\n",
    "# classifier.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR VALIDATION SET\n",
    "\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Group</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Group   1   2   3\n",
       "Actual Group               \n",
       "1                23   6   0\n",
       "2                 1  10   4\n",
       "3                 1   2  23"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Group'], colnames=['Predicted Group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88741722, 0.69444444, 0.89051095])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.76666667, 0.53571429, 0.75      , 0.75      , 0.82142857,\n",
       "       0.64285714, 0.71428571, 0.67857143, 0.7037037 , 0.56      ])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(classifier,X,y,cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['dwl_bytes_avg','dwl_peak_prom','upl_bytes_std','dwl_bytes_std','upl_peak_std']\n",
    "# importances = classifier.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# for i in indices:\n",
    "#     print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['dwl_bytes_avg','upl_max_psd','dwl_max_psd','upl_peak_prom','dwl_num_peak','dwl_peak_prom']\n",
    "# importances = classifier.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# for i in indices:\n",
    "#     print(features[i],': ',importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dwl_peak_prom :  0.13713088380289176\n",
      "dwl_num_peak :  0.12245980947453192\n",
      "dwl_max_psd :  0.11984258111516699\n",
      "dwl_time_peak :  0.11032937896712422\n",
      "upl_max_psd :  0.07179857958734272\n",
      "dwl_bytes_std :  0.06556642367311044\n",
      "upl_peak_prom :  0.06496325641984468\n",
      "dwl_bytes_avg :  0.04632395553127444\n",
      "upl_bytes_std :  0.044282375150057056\n",
      "dwl_peak_avg :  0.03945016693292953\n",
      "upl_bytes_avg :  0.03772954757472542\n",
      "dwl_peak_std :  0.03490470282877489\n",
      "upl_peak_avg :  0.030256294796682515\n",
      "upl_peak_std :  0.02789525781953951\n",
      "upl_peak_freq :  0.023926239489693947\n",
      "dwl_peak_freq :  0.023140546836309957\n"
     ]
    }
   ],
   "source": [
    "features = ['dwl_peak_freq','dwl_peak_prom','dwl_max_psd','dwl_bytes_avg','dwl_bytes_std','dwl_peak_avg',\n",
    "            'dwl_peak_std','upl_peak_freq','upl_peak_prom','upl_max_psd','upl_bytes_avg','upl_bytes_std',\n",
    "            'upl_peak_avg','upl_peak_std','dwl_time_peak','dwl_num_peak']\n",
    "importances = classifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing that feature method functions correctly\n",
    "\n",
    "# l_start = 0 \n",
    "# l_end = 60\n",
    "\n",
    "# test_chunk = stdoan_low.copy()\n",
    "# test_chunk['Time'] = test_chunk['Time'] - test_chunk['Time'].min()\n",
    "# low_chunk = stdoan_low[(stdoan_low['Time'] >= 0) & (stdoan_low['Time'] < 60)]\n",
    "\n",
    "# low_chunk_ms = convert_ms_df(low_chunk, True)\n",
    "\n",
    "# upl_ms = low_chunk_ms[low_chunk_ms['pkt_src'] == '1']\n",
    "# dwl_ms = low_chunk_ms[low_chunk_ms['pkt_src'] == '2']\n",
    "\n",
    "# dwl_chunk_rs = dwl_ms.resample('500ms', on='Time').sum()\n",
    "\n",
    "# f_dwl, Pxx_dwl = sp.signal.welch(dwl_chunk_rs['pkt_size'], fs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basline Model (3 resolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Group</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Group   1   3   5\n",
       "Actual Group               \n",
       "1                13   2   0\n",
       "3                 1  14   0\n",
       "5                 0   0  13"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BASELINE MODEL\n",
    "\n",
    "baseline_training = pd.concat([low_feat, med_feat, high_feat]).reset_index(drop=True)\n",
    "X, y = baseline_training.drop(columns=['resolution']), baseline_training['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4,stratify=baseline_training['resolution'])\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Group'], colnames=['Predicted Group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9302325581395349"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Group</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Group  1  3  5\n",
       "Actual Group            \n",
       "1                6  0  0\n",
       "3                0  6  0\n",
       "5                0  1  5"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN ON TEST DATA\n",
    "\n",
    "low_test = pd.read_csv('../data/test/sgs008-109-action-240p-20210202.csv')\n",
    "med_test = pd.read_csv('../data/test/sgs008-109-action-480p-20210202.csv')\n",
    "high_test = pd.read_csv('../data/test/sgs008-109-action-1080p-20210202.csv')\n",
    "# threesixty_test = pd.read_csv('../data/test/sgs008-109-action-360p-20210213.csv')\n",
    "# seventwenty_test = pd.read_csv('../data/test/sgs008-109-action-720p-20210213.csv')\n",
    "#low_test = pd.read_csv('../data/test/sgs008-109-action-360p-20210213.csv')\n",
    "#low_test = pd.read_csv('../data/test/stdoan-102-action-720p-20201206.csv')\n",
    "\n",
    "low_test_feat = create_features([low_test], chunk_size)\n",
    "#threesixty_test_feat = create_features([threesixty_test], chunk_size)\n",
    "med_test_feat = create_features([med_test], chunk_size)\n",
    "#seventwenty_test_feat = create_features([seventwenty_test], chunk_size)\n",
    "high_test_feat = create_features([high_test], chunk_size)\n",
    "\n",
    "low_test_feat['resolution'] = 1\n",
    "#threesixty_test_feat['resolution'] = 2\n",
    "med_test_feat['resolution'] = 3\n",
    "#seventwenty_test_feat['resolution'] = 4\n",
    "high_test_feat['resolution'] = 5\n",
    "\n",
    "test = pd.concat([low_test_feat, med_test_feat, high_test_feat])\n",
    "\n",
    "classifier.fit(X,y)\n",
    "y_pred = classifier.predict(test.drop('resolution',axis=1))\n",
    "(pd.crosstab(test['resolution'], y_pred, rownames=['Actual Group'], colnames=['Predicted Group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test['resolution'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
