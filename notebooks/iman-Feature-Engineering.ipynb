{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../src')\n",
    "from features import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbit_rate = 1/125000\n",
    "\n",
    "low_fp = '../data/240p/' \n",
    "med_fp = '../data/480p/'\n",
    "high_fp = '../data/1080p/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dfs = []\n",
    "for file in os.listdir(low_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        low_dfs.append(pd.read_csv(low_fp+file))\n",
    "    \n",
    "med_dfs = []\n",
    "for file in os.listdir(med_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        med_dfs.append(pd.read_csv(med_fp+file))\n",
    "    \n",
    "high_dfs = []\n",
    "for file in os.listdir(high_fp):\n",
    "    if file != '.ipynb_checkpoints' and file != '.DS_Store':\n",
    "        high_dfs.append(pd.read_csv(high_fp+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdoan_low = pd.read_csv(low_fp + 'stdoan-101-action-240p-20201127.csv')\n",
    "# stdoan_med = pd.read_csv(med_fp + 'stdoan-101-action-480p-20201127.csv')\n",
    "# stdoan_high = pd.read_csv(high_fp + 'stdoan-101-action-1080p-20201127.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_ms = []\n",
    "for df in low_dfs:\n",
    "    low_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "med_ms = []\n",
    "for df in med_dfs:\n",
    "    med_ms.append(convert_ms_df(df,True))\n",
    "    \n",
    "high_ms = []\n",
    "for df in high_dfs:\n",
    "    high_ms.append(convert_ms_df(df,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_ms = convert_ms_df(stdoan_low, True)\n",
    "# med_ms = convert_ms_df(stdoan_med, True)\n",
    "# high_ms = convert_ms_df(stdoan_high, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_resamples = []\n",
    "for df in low_ms:\n",
    "    low_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "med_resamples = []\n",
    "for df in med_ms:\n",
    "    med_resamples.append(df.resample('500ms', on='Time').sum())\n",
    "    \n",
    "high_resamples = []\n",
    "for df in high_ms:\n",
    "    high_resamples.append(df.resample('500ms', on='Time').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low_resample = low_ms.resample('500ms', on='Time').sum()\n",
    "# med_resample = med_ms.resample('500ms', on='Time').sum()\n",
    "# high_resample = high_ms.resample('500ms', on='Time').sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take the aggregate features of the whole chunk; download and upload\n",
    "def agg_feat(df, col):\n",
    "    return [np.mean(df[col]), np.std(df[col])]\n",
    "\n",
    "## take the ratio of upload:download packets\n",
    "def pkt_ratio(df):\n",
    "    ms_df = convert_ms_df(df, True)\n",
    "    local = np.sum(ms_df['pkt_src'] == '1') \n",
    "    server = np.sum(ms_df['pkt_src'] == '2') \n",
    "    return local / server\n",
    "\n",
    "## take the ratio of upload:download bytes\n",
    "def bytes_ratio(df):\n",
    "    local = df['1->2Bytes'].sum()\n",
    "    server = df['2->1Bytes'].sum()\n",
    "    return local / server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Related Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finds the peaks with mean + 2(1) std\n",
    "## run the above aggregate functions on the peaks only??\n",
    "\n",
    "def get_peak_loc(df, col, invert=False):\n",
    "    'invert arg allows you to get values not considered peaks'\n",
    "    df_avg = df[col].mean()\n",
    "    df_std = df[col].std()\n",
    "  \n",
    "    threshold = df_avg + (1 * df_std)\n",
    "  \n",
    "    if invert:\n",
    "        return np.array(df[col] < threshold)\n",
    "  \n",
    "    else:\n",
    "        return np.array(df[col] > threshold)\n",
    "\n",
    "## np.mean, np.var, np.std - think of more?  \n",
    "def peak_time_diff(df, col):\n",
    "    '''\n",
    "    mess around with the different inputs for function. \n",
    "    variance seems to inflate the difference betweent the two the most with litte\n",
    "    to no data manipulation. however, currently trying things like\n",
    "    squaring the data before taking the aggregate function to exaggerate\n",
    "    differences (moderate success??)\n",
    "    '''\n",
    "    peaks = df[get_peak_loc(df, col)]\n",
    "    peaks['Time'] = peaks['Time'] - peaks['Time'].min()\n",
    "    time_diff = np.diff(peaks['Time'] ** 2)\n",
    "    return [np.mean(time_diff), np.var(time_diff), np.std(time_diff)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "def peak_times(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    if list(peaks) == []:\n",
    "        return [-1]\n",
    "    times = df.iloc[peaks]['Time'].values\n",
    "    time_between_peaks = [times[i]-times[i-1]for i in range(1,len(times))]\n",
    "    #print(time_between_peaks)\n",
    "    #time_between_peaks[0]=0\n",
    "    if time_between_peaks == []:\n",
    "        return -1\n",
    "    return time_between_peaks\n",
    "\n",
    "def num_peaks(df,col,thresh):\n",
    "    x = df[col]\n",
    "    peaks, _ = find_peaks(x, height=thresh)\n",
    "    return len(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_features(df, col):\n",
    "    \"\"\"\n",
    "    welch implemention of spectral features\n",
    "    resample the data before inputting (might change prereq depending on\n",
    "    resource allocation)\n",
    "    \"\"\"\n",
    "    f, Pxx_den = sp.signal.welch(df[col], fs=2)\n",
    "    Pxx_den = np.sqrt(Pxx_den)\n",
    "\n",
    "    peaks = sp.signal.find_peaks(Pxx_den)[0]\n",
    "    prominences = sp.signal.peak_prominences(Pxx_den, peaks)[0]\n",
    "\n",
    "    idx_max = prominences.argmax()\n",
    "    loc_max = peaks[idx_max]\n",
    "\n",
    "    return [f[loc_max], Pxx_den[loc_max], prominences[idx_max]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking & Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## wip; need to decide chunk size eventually\n",
    "## should we also make this chunking feature be our feature creation?\n",
    "\n",
    "def chunk_data(df, interval=60):\n",
    "    \"\"\"\n",
    "    takes in a filepath to the data you want to chunk and feature engineer\n",
    "    chunks our data into a specified time interval\n",
    "    each chunk is then turned into an observation to be fed into our classifier\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    df['Time'] = df['Time'] - df['Time'].min()\n",
    "    \n",
    "    total_chunks = np.floor(df['Time'].max() / interval).astype(int)\n",
    "\n",
    "    for chunk in np.arange(total_chunks):\n",
    "      \n",
    "        start = chunk * interval\n",
    "        end = (chunk+1) * interval\n",
    "\n",
    "        temp_df = (df[(df['Time'] >= start) & (df['Time'] < end)])\n",
    "        \n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(dfs, interval=60):\n",
    "\n",
    "    features = [\n",
    "    'dwl_peak_freq',\n",
    "    'dwl_peak_prom',\n",
    "    'dwl_max_psd',\n",
    "    'dwl_bytes_avg',\n",
    "    'dwl_bytes_std',\n",
    "    'dwl_peak_avg',\n",
    "    'dwl_peak_var',\n",
    "    'dwl_peak_std',\n",
    "    'upl_peak_freq',\n",
    "    'upl_peak_prom',\n",
    "    'upl_max_psd',\n",
    "    'upl_bytes_avg',\n",
    "    'upl_bytes_std',\n",
    "    'upl_peak_avg',\n",
    "    'upl_peak_var',\n",
    "    'upl_peak_std',\n",
    "    'dwl_time_peak',#'IMAN_up_time_peak',\n",
    "      'dwl_num_peak'#,'IMAN_up_num_peak'\n",
    "    ]  \n",
    "\n",
    "    vals = []\n",
    "    for df in dfs:\n",
    "        df_chunks = chunk_data(df, interval)\n",
    "\n",
    "        for chunk in df_chunks:\n",
    "\n",
    "            preproc = convert_ms_df(chunk, True)\n",
    "            upl_bytes = preproc[preproc['pkt_src'] == '1'].resample('500ms', on='Time').sum()\n",
    "            dwl_bytes = preproc[preproc['pkt_src'] == '2'].resample('500ms', on='Time').sum()\n",
    "\n",
    "            ## spectral features\n",
    "            dwl_spectral = spectral_features(dwl_bytes, 'pkt_size')\n",
    "            upl_spectral = spectral_features(upl_bytes, 'pkt_size')\n",
    "\n",
    "            ## aggregate features\n",
    "            dwl_agg = agg_feat(chunk, '2->1Bytes')\n",
    "            upl_agg = agg_feat(chunk, '1->2Bytes')\n",
    "\n",
    "            ## peak features\n",
    "            dwl_peak = peak_time_diff(chunk, '2->1Bytes')\n",
    "            upl_peak = peak_time_diff(chunk, '1->2Bytes')\n",
    "\n",
    "            ## iman's time between peak \n",
    "            iman_dwn_time_peak = np.mean(peak_times(chunk,'2->1Bytes',1000000))\n",
    "            #iman_up_time_peak = np.mean(peak_times(chunk,'1->2Bytes',50000))\n",
    "\n",
    "            ## iman's num peak\n",
    "            iman_dwn_num_peak = num_peaks(chunk,'2->1Bytes',1000000)\n",
    "            #iman_up_num_peak = num_peaks(chunk,'1->2Bytes',50000)\n",
    "\n",
    "\n",
    "\n",
    "            feat_val = np.hstack((\n",
    "              dwl_spectral,\n",
    "              dwl_agg,\n",
    "              dwl_peak,\n",
    "              upl_spectral,\n",
    "              upl_agg,\n",
    "              upl_peak,\n",
    "                iman_dwn_time_peak,#iman_up_time_peak,\n",
    "                iman_dwn_num_peak,#iman_up_num_peak\n",
    "            ))\n",
    "\n",
    "            vals.append(feat_val)\n",
    "    \n",
    "    return pd.DataFrame(columns=features, data=vals).fillna(0)"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_four_script = os.listdir(two_four_fp)\n",
    "three_six_script = os.listdir(three_six_fp)\n",
    "four_eight_script = os.listdir(four_eight_fp)\n",
    "seven_two_script = os.listdir(seven_two_fp)\n",
    "ten_eight_script = os.listdir(ten_eight_fp)\n",
    "\n",
    "def add_resolution(fp, res):\n",
    "  temp_df = pd.read_csv(fp)\n",
    "  temp_df['resolution'] = res\n",
    "  return temp_df\n",
    "\n",
    "script_240_lst = [add_resolution(two_four_fp + fp, '240p') for fp in two_four_script]\n",
    "script_360_lst = [add_resolution(three_six_fp + fp, '360p') for fp in three_six_script]\n",
    "script_480_lst = [add_resolution(four_eight_fp + fp, '480p') for fp in four_eight_script]\n",
    "script_720_lst = [add_resolution(seven_two_fp + fp, '720p') for fp in seven_two_script]\n",
    "script_1080_lst = [add_resolution(ten_eight_fp + fp, '1080p') for fp in ten_eight_script]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_240_lst = sum([chunk_data(df) for df in script_240_lst], [])\n",
    "chunk_360_lst = sum([chunk_data(df) for df in script_360_lst], [])\n",
    "chunk_480_lst = sum([chunk_data(df) for df in script_480_lst], [])\n",
    "chunk_720_lst = sum([chunk_data(df) for df in script_720_lst], [])\n",
    "chunk_1080_lst = sum([chunk_data(df) for df in script_1080_lst], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "script_240_resample = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_240_lst]\n",
    "script_360_resample = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_360_lst]\n",
    "script_480_resample = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_480_lst]\n",
    "script_720_resample = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_720_lst]\n",
    "script_1080_resample = [convert_ms_df(df).resample(size, on='Time').sum() for df in chunk_1080_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_density(resample_lst, bins):\n",
    "  pxx_lst = []\n",
    "  for df in resample_lst:\n",
    "    f_temp, Pxx_temp = sp.signal.welch(df['pkt_size'], fs=.5) \n",
    "    freq = np.linspace(0, np.max(f_temp) + .01, num=bins) - .001\n",
    "    total = np.trapz(y=Pxx_temp, x=f_temp)\n",
    "    temp_lst = []\n",
    "    for i in np.arange(len(freq) - 1):\n",
    "      \n",
    "      f_lower = np.where(f_temp >= freq[i])\n",
    "      f_upper = np.where(f_temp < freq[i+1] )\n",
    "      selected_range = np.intersect1d(f_lower, f_upper)\n",
    "      \n",
    "      pxx_den = np.trapz(y=Pxx_temp[selected_range], x=f_temp[selected_range]) / total\n",
    "      \n",
    "      temp_lst.append(pxx_den)\n",
    "      \n",
    "    pxx_lst.append(temp_lst)  \n",
    "  return pxx_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = 4\n",
    "\n",
    "pxx_240 = power_density(script_240_resample, bin_size)\n",
    "pxx_360 = power_density(script_360_resample, bin_size)\n",
    "pxx_480 = power_density(script_480_resample, bin_size)\n",
    "pxx_720 = power_density(script_720_resample, bin_size)\n",
    "pxx_1080 = power_density(script_1080_resample, bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pxx_240, axis=0)\n",
    "np.mean(pxx_360, axis=0)\n",
    "np.mean(pxx_480, axis=0)\n",
    "np.mean(pxx_720, axis=0)\n",
    "np.mean(pxx_1080, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
=======
>>>>>>> 29b4d232e8706d1e2cda526ddad5bfa0465ab7b5
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.7 s, sys: 215 ms, total: 15.9 s\n",
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "low_feat = create_features(low_dfs, 100)\n",
    "med_feat = create_features(med_dfs, 100)\n",
    "high_feat = create_features(high_dfs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# low_feat = create_features(stdoan_low, 100)\n",
    "# med_feat = create_features(stdoan_med, 100)\n",
    "# high_feat = create_features(stdoan_high, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_feat['resolution'] = np.zeros(len(low_feat))\n",
    "med_feat['resolution'] = np.zeros(len(med_feat)) + 1\n",
    "high_feat['resolution'] = np.zeros(len(high_feat)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat([low_feat, med_feat, high_feat]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training[['dwl_bytes_avg','dwl_peak_prom','upl_bytes_std','dwl_bytes_std','upl_peak_std','resolution']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dwl_bytes_avg</th>\n",
       "      <th>dwl_peak_prom</th>\n",
       "      <th>upl_bytes_std</th>\n",
       "      <th>dwl_bytes_std</th>\n",
       "      <th>upl_peak_std</th>\n",
       "      <th>resolution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>114425.905263</td>\n",
       "      <td>2.559624e+05</td>\n",
       "      <td>17675.769254</td>\n",
       "      <td>305025.605472</td>\n",
       "      <td>803.776356</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>58461.855072</td>\n",
       "      <td>1.608469e+05</td>\n",
       "      <td>9327.334796</td>\n",
       "      <td>183186.907760</td>\n",
       "      <td>1260.907514</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>40925.519481</td>\n",
       "      <td>2.153807e+05</td>\n",
       "      <td>8072.574910</td>\n",
       "      <td>156778.675598</td>\n",
       "      <td>527.101983</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>51165.656250</td>\n",
       "      <td>1.678943e+05</td>\n",
       "      <td>10500.679157</td>\n",
       "      <td>212334.491065</td>\n",
       "      <td>1213.542839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>49224.350649</td>\n",
       "      <td>1.888234e+05</td>\n",
       "      <td>8511.764942</td>\n",
       "      <td>170981.928605</td>\n",
       "      <td>1326.941807</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>253738.413043</td>\n",
       "      <td>1.097268e+06</td>\n",
       "      <td>27622.507815</td>\n",
       "      <td>580189.681359</td>\n",
       "      <td>585.807349</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>217527.819444</td>\n",
       "      <td>9.765048e+05</td>\n",
       "      <td>24669.122888</td>\n",
       "      <td>516862.726467</td>\n",
       "      <td>1309.223625</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>229211.126437</td>\n",
       "      <td>5.251847e+05</td>\n",
       "      <td>27504.462558</td>\n",
       "      <td>575108.238130</td>\n",
       "      <td>904.036526</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>221431.108108</td>\n",
       "      <td>8.523427e+05</td>\n",
       "      <td>27438.619912</td>\n",
       "      <td>578239.976452</td>\n",
       "      <td>414.244927</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>212427.789474</td>\n",
       "      <td>1.039235e+06</td>\n",
       "      <td>24536.179618</td>\n",
       "      <td>514930.758124</td>\n",
       "      <td>515.193599</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dwl_bytes_avg  dwl_peak_prom  upl_bytes_std  dwl_bytes_std  upl_peak_std  \\\n",
       "0    114425.905263   2.559624e+05   17675.769254  305025.605472    803.776356   \n",
       "1     58461.855072   1.608469e+05    9327.334796  183186.907760   1260.907514   \n",
       "2     40925.519481   2.153807e+05    8072.574910  156778.675598    527.101983   \n",
       "3     51165.656250   1.678943e+05   10500.679157  212334.491065   1213.542839   \n",
       "4     49224.350649   1.888234e+05    8511.764942  170981.928605   1326.941807   \n",
       "..             ...            ...            ...            ...           ...   \n",
       "130  253738.413043   1.097268e+06   27622.507815  580189.681359    585.807349   \n",
       "131  217527.819444   9.765048e+05   24669.122888  516862.726467   1309.223625   \n",
       "132  229211.126437   5.251847e+05   27504.462558  575108.238130    904.036526   \n",
       "133  221431.108108   8.523427e+05   27438.619912  578239.976452    414.244927   \n",
       "134  212427.789474   1.039235e+06   24536.179618  514930.758124    515.193599   \n",
       "\n",
       "     resolution  \n",
       "0           0.0  \n",
       "1           0.0  \n",
       "2           0.0  \n",
       "3           0.0  \n",
       "4           0.0  \n",
       "..          ...  \n",
       "130         2.0  \n",
       "131         2.0  \n",
       "132         2.0  \n",
       "133         2.0  \n",
       "134         2.0  \n",
       "\n",
       "[135 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training.drop(columns=['resolution']), training['resolution']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=8, random_state=42)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 8, criterion = 'entropy', random_state = 42)\n",
    "#classifier.fit(X_train, y_train)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_test = pd.read_csv('../data/test/sgs008-109-action-480p-20210202.csv')\n",
    "low_feat = create_features([low_test], 100)\n",
    "low_feat['resolution']=np.zeros(len(low_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_low = low_feat[['dwl_bytes_avg','dwl_peak_prom','upl_bytes_std','dwl_bytes_std','upl_peak_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred = classifier.predict(test_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 1., 2., 2., 2., 1., 0., 0., 0., 0., 2.,\n",
       "       2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 1., 1., 0., 2., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Group</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Group  0.0  1.0  2.0\n",
       "Actual Group                  \n",
       "0.0               19    0    0\n",
       "1.0                1    6    0\n",
       "2.0                0    0    8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pd.crosstab(y_test, y_pred, rownames=['Actual Group'], colnames=['Predicted Group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97435897, 0.92307692, 1.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9705882352941176"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dwl_peak_prom :  0.319308713921942\n",
      "dwl_bytes_avg :  0.2830167276765433\n",
      "dwl_bytes_std :  0.23012542420827098\n",
      "upl_bytes_std :  0.14934340970172777\n",
      "upl_peak_std :  0.01820572449151598\n"
     ]
    }
   ],
   "source": [
    "features = ['dwl_bytes_avg','dwl_peak_prom','upl_bytes_std','dwl_bytes_std','upl_peak_std']\n",
    "importances = classifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in indices:\n",
    "    print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# features = ['dwl_peak_freq','dwl_peak_prom','dwl_max_psd','dwl_bytes_avg','dwl_bytes_std','dwl_peak_avg',\n",
    "#     'dwl_peak_var','dwl_peak_std','upl_peak_freq','upl_peak_prom','upl_max_psd','upl_bytes_avg','upl_bytes_std','upl_peak_avg','upl_peak_var','upl_peak_std',\n",
    "#     'dwl_time_peak',#'IMAN_up_time_peak',\n",
    "#       'dwl_num_peak']\n",
    "# importances = classifier.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# for i in indices:\n",
    "#     print(features[i],': ',importances[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## testing that feature method functions correctly\n",
    "\n",
    "# l_start = 0 \n",
    "# l_end = 60\n",
    "\n",
    "# test_chunk = stdoan_low.copy()\n",
    "# test_chunk['Time'] = test_chunk['Time'] - test_chunk['Time'].min()\n",
    "# low_chunk = stdoan_low[(stdoan_low['Time'] >= 0) & (stdoan_low['Time'] < 60)]\n",
    "\n",
    "# low_chunk_ms = convert_ms_df(low_chunk, True)\n",
    "\n",
    "# upl_ms = low_chunk_ms[low_chunk_ms['pkt_src'] == '1']\n",
    "# dwl_ms = low_chunk_ms[low_chunk_ms['pkt_src'] == '2']\n",
    "\n",
    "# dwl_chunk_rs = dwl_ms.resample('500ms', on='Time').sum()\n",
    "\n",
    "# f_dwl, Pxx_dwl = sp.signal.welch(dwl_chunk_rs['pkt_size'], fs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_search = {\n",
    "  'bootstrap': [True, False],\n",
    "  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "  'max_features': ['auto', 'sqrt'],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
